# Libs Metric

[_Documentation generated by Documatic_](https://www.documatic.com)

<!---Documatic-section-Codebase Structure-start--->
## Codebase Structure

<!---Documatic-block-system_architecture-start--->
```mermaid
None
```
<!---Documatic-block-system_architecture-end--->

# #
<!---Documatic-section-Codebase Structure-end--->

<!---Documatic-section-libs.metric.confusion_matrix-start--->
## [libs.metric.confusion_matrix](3-libs_metric.md#libs.metric.confusion_matrix)

<!---Documatic-section-confusion_matrix-start--->
<!---Documatic-block-libs.metric.confusion_matrix-start--->
<details>
	<summary><code>libs.metric.confusion_matrix</code> code snippet</summary>

```python
def confusion_matrix(pred, label, num_classes):
    mask = (label >= 0) & (label < num_classes)
    conf_mat = np.bincount(num_classes * label[mask].astype(int) + pred[mask], minlength=num_classes ** 2).reshape(num_classes, num_classes)
    return conf_mat
```
</details>
<!---Documatic-block-libs.metric.confusion_matrix-end--->
<!---Documatic-section-confusion_matrix-end--->

# #
<!---Documatic-section-libs.metric.confusion_matrix-end--->

<!---Documatic-section-libs.metric.evaluate-start--->
## [libs.metric.evaluate](3-libs_metric.md#libs.metric.evaluate)

<!---Documatic-section-evaluate-start--->
<!---Documatic-block-libs.metric.evaluate-start--->
<details>
	<summary><code>libs.metric.evaluate</code> code snippet</summary>

```python
def evaluate(conf_mat):
    acc = np.diag(conf_mat).sum() / conf_mat.sum()
    acc_per_class = np.diag(conf_mat) / conf_mat.sum(axis=1)
    acc_cls = np.nanmean(acc_per_class)
    IoU = np.diag(conf_mat) / (conf_mat.sum(axis=1) + conf_mat.sum(axis=0) - np.diag(conf_mat))
    mean_IoU = np.nanmean(IoU)
    pe = np.dot(np.sum(conf_mat, axis=0), np.sum(conf_mat, axis=1)) / conf_mat.sum() ** 2
    kappa = (acc - pe) / (1 - pe)
    return (acc, acc_per_class, acc_cls, IoU, mean_IoU, kappa)
```
</details>
<!---Documatic-block-libs.metric.evaluate-end--->
<!---Documatic-section-evaluate-end--->

# #
<!---Documatic-section-libs.metric.evaluate-end--->

[_Documentation generated by Documatic_](https://www.documatic.com)